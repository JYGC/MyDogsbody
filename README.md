The is an attempt to create a web crawler. My intention is to
eventually hand a tool which is capable of retrieving information
not only form the websites already crawled by search engines but
also the deep web.

Right now it is does not have that capability although I'm hopping
to add it in the near future. But right now, there are still many
problems that have to be ironed out first.

Contributions, forks and derivatives are welcome.

Dependencies:
- ruby 1.9
- rubygem-nokogiri

############ Change Log: 28/06/14 ###############
Message: Re-organising some code

Rewritten some code, seperate code accourding to
their function.

############ Change Log: 24/06/14 ###############
Message: Rewritten in Ruby

Originally, this program was going to be written
using a variety of programming languages such as
bash shell, perl and php. However, such approaches
result in a system that is rather difficult to
manage compared to an equivalent system that only
uses one programming language.

I have therefore rewritten the entire thing using
ruby 1.9. However, I may end up add some perl
modules to the system as there may be situations
in which perl's runtime performance speed is needed.
